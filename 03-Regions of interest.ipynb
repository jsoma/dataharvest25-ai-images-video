{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3710f31-d315-48c0-8e50-6642b0338646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade transformers timm supervision tqdm \"scenedetect[opencv]\" pillow einops\n",
    "%pip install --quiet --upgrade openai \"google-genai\" pydantic pandas \n",
    "%pip install --quiet --upgrade yt-dlp ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa173100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEYS GO HERE, ASK SOMA!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0634b",
   "metadata": {},
   "source": [
    "# Regions of interest\n",
    "\n",
    "Oftentimes you don't want to analyze the *whole* video or the *whole* image. For example, in our last notebook we just looked at the middle video in each scene. That's called a **region of interest.** By adding in an extra step that cuts out unwanted information (or selects wanted information) you can often use cheaper, faster, or less powerful tools later on in the process.\n",
    "\n",
    "For videos regions of interest are usually thought of as **time**, and for images it's usually thought of as **objects**.\n",
    "\n",
    "## Downloading our video\n",
    "\n",
    "Let's download that same video *again*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=rDXubdQdJYs\"\n",
    "\n",
    "# 720p or less\n",
    "ydl_opts = {\n",
    "    'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',\n",
    "    'outtmpl': '%(id)s.%(ext)s',\n",
    "    'merge_output_format': 'mp4',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegVideoConvertor',\n",
    "        'preferedformat': 'mp4',  # force re-encode into Quick Look–friendly format\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076104fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"rDXubdQdJYs.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f15ed4",
   "metadata": {},
   "source": [
    "## Selecting times based on transcripts\n",
    "\n",
    "When DocumentedNY analyzed [misinformation on TikTok](https://pulitzercenter.org/misinformation-tiktok-how-documented-examined-hundreds-videos-different-languages) they didn't need to actually analyze the video: the transcript did almost all of the work!\n",
    "\n",
    "Turning audio into easily parseable text is a great way to filter down a long stretch of video into just the portions you want. Instead of watching a million videos start-to-finish of RFK Jr. or Pete Hegseth talking on podcasts and Fox News, techniques like this allow you to quickly narrow down your target (and *then* maybe use some image- or video-based AI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9bcc6-adc9-444f-98dc-9747ed3f2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade openai-whisper ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbedec7",
   "metadata": {},
   "source": [
    "We'll start by using ffmpeg to convert our video to an mp3. While we *could* have downloaded just the audio, we want to slice up the video later do we took it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77698043-9230-4f47-8cb6-215e385cc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = ffmpeg.input('rDXubdQdJYs.mp4').output(\n",
    "    'rDXubdQdJYs.mp3',\n",
    "    vn=None,          # no video\n",
    "    acodec='libmp3lame'\n",
    ").run(quiet=True, overwrite_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20280f",
   "metadata": {},
   "source": [
    "Now we'll use [Whisper](https://github.com/openai/whisper) to transcribe it. I usually use [WhisperX](https://github.com/m-bain/whisperX) but i've had weird setup problems on other peoples' computers so we'll stick with the default.\n",
    "\n",
    "Whisper comes in several sizes, with smaller models being faster and less effective. The newest one called `turbo` is a great combination of speed and accuracy.\n",
    "\n",
    "> You [can't trust transcription](https://apnews.com/article/ai-artificial-intelligence-health-business-90020cdf5fa16c79ca2e5b6c4c9bbb14), but you're not an idiot so you're going to be watching these clips regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb6e78-6d76-4680-ad9c-e64730466f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"turbo\")\n",
    "result = model.transcribe(\"rDXubdQdJYs.mp3\")\n",
    "text = result['text']\n",
    "text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2ab1e",
   "metadata": {},
   "source": [
    "Along with the transcribed text of the audio, Whisper also provides timestamped segments. We can use that to filter for specific segments of the video!\n",
    "\n",
    "## Time-based regions of interest\n",
    "\n",
    "First we'll throw the **segments** into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c97d0d-5200-4bfc-acee-02bcb037a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "df = pd.DataFrame(result['segments'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fb072",
   "metadata": {},
   "source": [
    "Trump loved to talk about Hunter Biden, Joe Biden's son. Let's filter for `son` to see how that went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d60b2-39fb-414d-97e3-ef91f9b7496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = df[df['text'].str.contains(\"son\")]\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cbb9b",
   "metadata": {},
   "source": [
    "While we can guess who's speaking at each of those moments, *can we really know?* And even if the answer is \"yes,\" don't we want to use some AI to verify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9820b",
   "metadata": {},
   "source": [
    "Cut out those sections (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3394ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "\n",
    "filename = 'rDXubdQdJYs.mp4'\n",
    "output = f'{filename}_frames'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output, exist_ok=True)\n",
    "\n",
    "# Loop through rows in the DataFrame\n",
    "for i, row in selected.iterrows():\n",
    "    midpoint = (row['start'] + row['end']) / 2\n",
    "    output_path = os.path.join(\n",
    "        output, \n",
    "        f\"frame_{i:03d}.jpg\"\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(filename, ss=midpoint)\n",
    "        .output(output_path, vframes=1)\n",
    "        .run(quiet=True, overwrite_output=True)\n",
    "    )\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c3197",
   "metadata": {},
   "source": [
    "Cut out those sections (video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa121648-de1d-4fb7-8284-62fe1330dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "\n",
    "filename = 'rDXubdQdJYs.mp4'\n",
    "output = f'{filename}_clips'\n",
    "buffer_seconds = 2  # seconds\n",
    "\n",
    "os.makedirs(output, exist_ok=True)\n",
    "\n",
    "# Loop through each row to generate clips\n",
    "for idx, row in selected.iterrows():\n",
    "    start = max(row['start'] - buffer_seconds, 0)\n",
    "    end = row['end'] + buffer_seconds\n",
    "    duration = end - start\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        output,\n",
    "        f\"clip_{row['id']}_{int(start)}-{int(end)}.mp4\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(filename, ss=start, t=duration)\n",
    "        .output(output_path, c='copy')  # Fast copy (no re-encode)\n",
    "        .run(quiet=True, overwrite_output=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49d58b",
   "metadata": {},
   "source": [
    "Just kidding! But if you want to do the classification with either LLMs or transformers you're welcome to, the images are *right there waiting for you*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83d353-ff56-4cca-8247-97af6cc3b67c",
   "metadata": {},
   "source": [
    "## Space-based regions of interest\n",
    "\n",
    "If you're looking at an individual image, you might only be interested in **part of the image**. Things like, are there guns in this image? Does this protest contain hate symbols on flags? How many journalists can fit in Bar Populaire?\n",
    "\n",
    "These kinds of questions involve **object detection**, the process of...... detecting objects. You can [read more here](https://huggingface.co/docs/transformers/en/tasks/object_detection).\n",
    "\n",
    "Again, we're going to use a **zero shot model**. We aren't doing anything specific or weird – only general stuff – so it should know what we're talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085e39d-ef1d-45bd-86b0-6e98f1a0b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7e7b2",
   "metadata": {},
   "source": [
    "While we *could* grab Trump and Biden's faces, let's try something else. **Cars!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be343f-d1ce-4284-8ebe-afc51de2568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"cars/28246634.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee275fe",
   "metadata": {},
   "source": [
    "What can we find in the image? Wheels? License plates? Vehicles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2c022-57fb-48a7-b282-c7d4dfa057ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = detector(\n",
    "    image,\n",
    "    candidate_labels=[\"wheel\", \"license plate\", \"vehicle\"],\n",
    ")\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8f322",
   "metadata": {},
   "source": [
    "We'll use PIL to draw some boxes and let us know what we're seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "annotated = image.copy()\n",
    "draw = ImageDraw.Draw(annotated)\n",
    "\n",
    "for prediction in predictions:\n",
    "    box = prediction[\"box\"]\n",
    "    label = prediction[\"label\"]\n",
    "    score = prediction[\"score\"]\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box.values()\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "    draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "annotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143bf3f",
   "metadata": {},
   "source": [
    "**That looks awful**. Let's try filtering for things with a score above 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c17bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "annotated = image.copy()\n",
    "draw = ImageDraw.Draw(annotated)\n",
    "\n",
    "good_predictions = [pred for pred in predictions if pred['score'] > 0.5]\n",
    "for prediction in good_predictions:\n",
    "    box = prediction[\"box\"]\n",
    "    label = prediction[\"label\"]\n",
    "    score = prediction[\"score\"]\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box.values()\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "    draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "annotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b6ea8",
   "metadata": {},
   "source": [
    "Perfect! If we're just interested in that area, we can crop it out by using the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = good_predictions[0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [\n",
    "    prediction['box']['xmin'],\n",
    "    prediction['box']['ymin'],\n",
    "    prediction['box']['xmax'],\n",
    "    prediction['box']['ymax'],\n",
    "]\n",
    "cropped = image.crop(bbox)\n",
    "cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee9405",
   "metadata": {},
   "source": [
    "Maybe since we cut out the rest of the image, we now have a nice tiny section we can send to an LLM for describing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Just ask \"write me a Pydantic model for XXXX\"\n",
    "class ImageDescription(BaseModel):\n",
    "    country_guess: str = Field(\"Best guess of the country the license plate is from\")\n",
    "    plate_number: str = Field(\"Text of license plate number\")\n",
    "    additional_notes: str\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "def ask_llm_cropped(cropped):\n",
    "    buffer = BytesIO()\n",
    "    cropped.save(buffer, format=\"PNG\")\n",
    "    b64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gemini-2.5-flash-preview-05-20\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format=ImageDescription,\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.parsed\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c59d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llm_cropped(cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c1ff9",
   "metadata": {},
   "source": [
    "And again, **we can just do it for a series of images**. Let's crop out all of the license plates in all of the `cars` folder and move them into their own folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b78f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "output_dir = Path(\"cars_license_plates\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "filenames = glob.glob(\"cars/*.jpg\")\n",
    "\n",
    "results = []\n",
    "for filename in filenames:\n",
    "    image = Image.open(filename)\n",
    "    predictions = detector(\n",
    "        image,\n",
    "        candidate_labels=[\"license plate\"],\n",
    "    )\n",
    "\n",
    "    base = Path(filename).stem  # filename without extension\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction['score'] > 0.5:\n",
    "            box = prediction[\"box\"]\n",
    "            xmin, ymin, xmax, ymax = box.values()\n",
    "            cropped = image.crop((xmin, ymin, xmax, ymax))\n",
    "            cropped_filename = output_dir / f\"{base}_{i+1}.jpg\"\n",
    "            cropped.save(cropped_filename)\n",
    "            \n",
    "            result = prediction.copy()\n",
    "            result['filename'] = filename\n",
    "            result['cropped'] = cropped_filename\n",
    "            results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5514b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(results)\n",
    "df['preview'] = df['cropped'].apply(lambda filename: f'<img src=\"{filename}\" width=\"100\"/>')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(df.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22def9",
   "metadata": {},
   "source": [
    "I didn't run `ask_llm_cropped` in the interests of finishing the notebook, but adding it might be a good exercise for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e58fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7f3ec9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

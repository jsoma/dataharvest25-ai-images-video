{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac90a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch supervision inference pillow google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jsoma/dataharvest25-ai-images-video/main/istockphoto-534232220-640_adpp_is.mp4\"\n",
    "urllib.request.urlretrieve(url, \"istockphoto-534232220-640_adpp_is.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3126a-37ac-4bee-929f-8bdb2e2d3e54",
   "metadata": {},
   "source": [
    "## Video processing: Car tracking\n",
    "\n",
    "Want to reproduce [this Bloomberg piece about congestion pricing?](https://www.bloomberg.com/graphics/2025-nyc-congestion-pricing-week-one-traffic-mix-shifts/) We can get about... 60% of the way there!\n",
    "\n",
    "This code is based heavily on [Piotr Skalski's presentation](https://x.com/i/broadcasts/1YqKDkdMOOYxV) at MIT from Nov 14, 2024. His is a good bit more interesting, though, as it talks about extracting license plates, OCRing them and tracking their movement in and out of a parking lot.\n",
    "\n",
    "The library we're using for most of this is [supervision](https://supervision.roboflow.com/latest/), which is very very very tied to [Roboflow](https://roboflow.com/). Think of Roboflow as the the Hugging Face of vision models, except Hugging Face *does* have vision models and Roboflow's website is *not that easy to use*. Supervision is a fantastic library, though, so we're using it!\n",
    "\n",
    "> If you want to use Hugging Face + pipelines + whatever models from over there, you just need to convert your detects into a supervision-friendly format. I just print out what my model gives me and ask an LLM.\n",
    "\n",
    "\n",
    "## Our video\n",
    "\n",
    "We're starting from a wonderful video from iStockPhoto because I don't have a bunch of videos of traffic laying around (sorry).\n",
    "\n",
    "<div style=\"text-align: center\"><video src=\"istockphoto-534232220-640_adpp_is.mp4\" controls loop autoplay></div>\n",
    "\n",
    "Let's see what we can do with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758f7dd-649e-4ebc-a3c9-d4d7ea5fb9df",
   "metadata": {},
   "source": [
    "## Add detecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4b94d-a637-44e2-820b-43b2ecfb4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "\n",
    "model = get_model(\"yolov10n-640\")\n",
    "\n",
    "video_path = \"istockphoto-534232220-640_adpp_is.mp4\"\n",
    "frame_generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "try:\n",
    "    for frame in frame_generator:\n",
    "        result = model.infer(frame)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        annotated_frame = box_annotator.annotate(annotated_frame, detections)\n",
    "    \n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections)\n",
    "    \n",
    "        # Display the image\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(annotated_frame))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c91912-1be4-487f-922f-b7915fc4f4fd",
   "metadata": {},
   "source": [
    "## Add tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08f0c7-f481-4225-a5c4-8b1c0f4eb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "\n",
    "model = get_model(\"yolov10n-640\")\n",
    "\n",
    "video_path = \"istockphoto-534232220-640_adpp_is.mp4\"\n",
    "frame_generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "byte_track = sv.ByteTrack()\n",
    "byte_track.reset()\n",
    "\n",
    "trace_annotator = sv.TraceAnnotator()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "try:\n",
    "    for frame in frame_generator:\n",
    "        result = model.infer(frame, confidence=0.3)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = byte_track.update_with_detections(detections)\n",
    "        detections = smoother.update_with_detections(detections)\n",
    "        \n",
    "        annotated_frame = frame.copy()\n",
    "        labels = [\n",
    "            f\"{model.class_names[class_id]} {confidence:0.2f}\"\n",
    "            for _, _, confidence, class_id, tracker_id, _\n",
    "            in detections\n",
    "        ]\n",
    "        \n",
    "        annotated_frame = box_annotator.annotate(annotated_frame, detections)\n",
    "        annotated_frame = trace_annotator.annotate(annotated_frame, detections)\n",
    "    \n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels)\n",
    "    \n",
    "        # Display the image\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(annotated_frame))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d25a54-97d8-465f-839c-c662a8a273b0",
   "metadata": {},
   "source": [
    "## Add counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6892e0d-5a52-4f65-a0eb-d1bd367746d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "\n",
    "model = get_model(\"yolov10n-640\")\n",
    "\n",
    "video_path = \"istockphoto-534232220-640_adpp_is.mp4\"\n",
    "frame_generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "line_zone_annotator = sv.LineZoneAnnotator(text_thickness=1)\n",
    "\n",
    "byte_track = sv.ByteTrack()\n",
    "byte_track.reset()\n",
    "\n",
    "start = sv.Point(200, 175)\n",
    "end = sv.Point(700, 175)\n",
    "line_zone = sv.LineZone(start, end)\n",
    "trace_annotator = sv.TraceAnnotator()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "try:\n",
    "    for frame in frame_generator:\n",
    "        result = model.infer(frame, confidence=0.3)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = byte_track.update_with_detections(detections)\n",
    "        detections = smoother.update_with_detections(detections)\n",
    "    \n",
    "        line_zone.trigger(detections)\n",
    "    \n",
    "        annotated_frame = frame.copy()\n",
    "        labels = [\n",
    "            f\"#{tracker_id} {model.class_names[class_id]} {confidence:0.2f}\"\n",
    "            for _, _, confidence, class_id, tracker_id, _\n",
    "            in detections\n",
    "        ]\n",
    "        \n",
    "        annotated_frame = box_annotator.annotate(annotated_frame, detections)\n",
    "        annotated_frame = trace_annotator.annotate(annotated_frame, detections)\n",
    "    \n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels)\n",
    "    \n",
    "        annotated_frame = line_zone_annotator.annotate(\n",
    "            annotated_frame,\n",
    "            line_counter=line_zone)\n",
    "    \n",
    "        # Display the image\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(annotated_frame))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf046ba1-2048-4f31-b99e-1427f1d31eb5",
   "metadata": {},
   "source": [
    "## Final version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f620eb-cf21-4d80-b102-0ad714eb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "\n",
    "model = get_model(\"yolov10n-640\")\n",
    "\n",
    "video_path = \"istockphoto-534232220-640_adpp_is.mp4\"\n",
    "frame_generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "line_zone_annotator = sv.LineZoneAnnotator(text_thickness=1)\n",
    "\n",
    "byte_track = sv.ByteTrack()\n",
    "byte_track.reset()\n",
    "\n",
    "start = sv.Point(200, 175)\n",
    "end = sv.Point(700, 175)\n",
    "line_zone = sv.LineZone(start, end)\n",
    "trace_annotator = sv.TraceAnnotator()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "try:\n",
    "    for frame in frame_generator:\n",
    "        result = model.infer(frame, confidence=0.3)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = byte_track.update_with_detections(detections)\n",
    "        detections = smoother.update_with_detections(detections)\n",
    "    \n",
    "        line_zone.trigger(detections)\n",
    "    \n",
    "        annotated_frame = frame.copy()\n",
    "        labels = [\n",
    "            f\"#{tracker_id} {model.class_names[class_id]} {confidence:0.2f}\"\n",
    "            for _, _, confidence, class_id, tracker_id, _\n",
    "            in detections\n",
    "        ]\n",
    "        \n",
    "        annotated_frame = box_annotator.annotate(annotated_frame, detections)\n",
    "        annotated_frame = trace_annotator.annotate(annotated_frame, detections)\n",
    "    \n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels)\n",
    "    \n",
    "        annotated_frame = line_zone_annotator.annotate(\n",
    "            annotated_frame,\n",
    "            line_counter=line_zone)\n",
    "    \n",
    "        # Display the image\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(annotated_frame))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288edd3-8577-4737-8a18-a008c2c4490d",
   "metadata": {},
   "source": [
    "## Going further!\n",
    "\n",
    "What if you don't just want cars, though â€“ you want to be *Bloomberg*, and spot taxis and box trucks and commercial vehicles and all sorts of specific cars! While you might try to find a model that does that for you, you can also **train your own** with a handful of pictures and videos.\n",
    "\n",
    "If you're interested in going to the next level instead of just using out-of-the-box models, check out [the Roboflow documentation](https://blog.roboflow.com/getting-started-with-roboflow/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82b04e-3310-4bd3-97ba-2e61001f73bc",
   "metadata": {},
   "source": [
    "# ...but also: can we do it with Gemini?\n",
    "\n",
    "We're going to steal the code from [the video understanding notebook](01-Video%20understanding%20with%20Google%20Gemini.ipynb) and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122ad73-f6f2-4ed7-8083-79539c8a385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key='GEMINI API KEY GOES HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8de34c-1ee0-43c0-8ee2-a3251c6c8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_video(video_file_name):\n",
    "  video_file = client.files.upload(file=video_file_name)\n",
    "\n",
    "  while video_file.state == \"PROCESSING\":\n",
    "      print('Waiting for video to be processed.')\n",
    "      time.sleep(10)\n",
    "      video_file = client.files.get(name=video_file.name)\n",
    "\n",
    "  if video_file.state == \"FAILED\":\n",
    "    raise ValueError(video_file.state)\n",
    "  print(f'Video processing complete: ' + video_file.uri)\n",
    "\n",
    "  return video_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e5f19-4818-48e0-bad8-4e84dbf2d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = upload_video(\"istockphoto-534232220-640_adpp_is.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d6f22-c660-45a0-a1e0-407584a31f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "Watch the traffic video. Provide a count of the number of cars that \n",
    "go into the left tunnel and the number of cars that come out of \n",
    "the right tunnel.\n",
    "\"\"\"\n",
    "\n",
    "video = video\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\n",
    "        video,\n",
    "        prompt,\n",
    "    ],\n",
    "    config=GenerateContentConfig(temperature=0)\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334dea6-b57d-4870-91f8-b0961b4cf058",
   "metadata": {},
   "source": [
    "**The actual number is around 20 and 20,** so no! Let's maybe not trust the all-knowing AI with this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae727e75-f1fa-4e5c-9992-98afa8e9eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI with images and video",
   "language": "python",
   "name": "images-video-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

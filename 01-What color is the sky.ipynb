{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3710f31-d315-48c0-8e50-6642b0338646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade openai pillow pydantic pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a179f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jsoma/dataharvest25-ai-images-video/main/sky.jpg\"\n",
    "urllib.request.urlretrieve(url, \"sky.jpg\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jsoma/dataharvest25-ai-images-video/main/city.png\"\n",
    "urllib.request.urlretrieve(url, \"city.png\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jsoma/dataharvest25-ai-images-video/main/cars.zip\"\n",
    "urllib.request.urlretrieve(url, \"cars.zip\")\n",
    "\n",
    "with zipfile.ZipFile('cars.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702ded5-7756-42de-bf05-ed201e9778a4",
   "metadata": {},
   "source": [
    "We're going to start our journey by talking **directly to large language models**. They don't involve any heavy lifting or fancy installations on our part, they're remarkably useful across different types of problems, and you don't need many technical skills to get a \"good enough\" solution.\n",
    "\n",
    "## Our LLM options\n",
    "\n",
    "When we talk about LLMs, we think of them as chatbots that are great at text. And that's usually true! If we want one that processes other types of content – images, video or audio – we are looking for a **multimodal model**. Multi-modal just means it takes different *modes* of input beyond text.\n",
    "\n",
    "Model availabilities and capabilities change over time, as does pricing (the most important part!). Down below we'll play around with different providers *and* different models, but you can also browse the documentation to see what models might work with your use case.\n",
    "\n",
    "### OpenAI's GPT\n",
    "\n",
    "- https://platform.openai.com/docs/models/compare\n",
    "- https://platform.openai.com/docs/pricing\n",
    "- https://platform.openai.com/docs/guides/images-vision#analyze-images\n",
    "\n",
    "### Google Gemini\n",
    "\n",
    "- https://ai.google.dev/gemini-api/docs/models\n",
    "- https://aistudio.google.com/prompts/new_chat\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/video-understanding\n",
    "\n",
    "### Anthropic's Claude\n",
    "\n",
    "- https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table\n",
    "\n",
    "### Deepseek\n",
    "\n",
    "I love it but it's just *so slow* when you're using it through the API.\n",
    "\n",
    "# Using the LLMs directly\n",
    "\n",
    "We'll start with talking directly to the LLMs.\n",
    "\n",
    "## Basic requests\n",
    "\n",
    "While each LLM provider has their own tools and libraries to work with the LLM, they almost all support an \"OpenAI compatible endpoint.\" Since ChatGPT got very popular very quickly, this is an attempt by Anthropic and Google to ease the transition to using their tooling.\n",
    "\n",
    "This means means you can re-use a lot of your code across different providers easily.\n",
    "\n",
    "Find more about the library here: https://github.com/openai/openai-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the openai library\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aeff9a",
   "metadata": {},
   "source": [
    "In order to use these services we need **API keys!**\n",
    "\n",
    "While using the AI chatbots through the website is almost always free, using them through *Python* costs money. API keys are how they track your usage. In this case, just use mine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa173100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEYS GO HERE, ASK SOMA!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe9d72",
   "metadata": {},
   "source": [
    "### Talking to ChatGPT\n",
    "\n",
    "Here is how you talk to ChatGPT. You let it know the model you want, a series of messages, and then print out the (very awkward) `completion.choices[0].message.content`. There's actually a more recent version of the API buuuut it doesn't work with other providers so we're ignoring it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c7acd-17f6-4d37-ba89-0caeb7b96ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": \"What color is the sky?\", },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb644ad",
   "metadata": {},
   "source": [
    "### Talking to Claude\n",
    "\n",
    "Claude, the AI model run by Anthropic, is known for having the *best personality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080a341-ef29-4ecd-9911-4d46e8984101",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url='https://api.anthropic.com/v1/', api_key=CLAUDE_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": \"What color is the sky?\", },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c21f02",
   "metadata": {},
   "source": [
    "### Talking to Google Gemini\n",
    "\n",
    "Everyone ignores Google Gemini, even though it works *great*. It can be a little wordy sometimes but it's very very capable (as you'll see later on).\n",
    "\n",
    "To experiment with it, you should *not* use the normal chat interface, but instead [AI Studio](https://aistudio.google.com/app/u/0/prompts/new_chat?pli=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7d173-dd2f-411c-829d-cf3b156f8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": \"What color is the sky?\", },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dcb237-1531-45ec-b9ee-d5c0f5f6cb5d",
   "metadata": {},
   "source": [
    "## Making image requests\n",
    "\n",
    "So far we've been making basic text requests. Since this session is all about images and video we're now going to upgrade to working with images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5c5f8-ac1d-47c0-b8c1-1481ae1da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from IPython.display import Image\n",
    "\n",
    "filename = 'sky.jpg'\n",
    "\n",
    "Image(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ab0ab",
   "metadata": {},
   "source": [
    "The only complicated part of using images with an LLM is **converting them to base64 encoding,** a representation of the image using printable characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c6815-d489-469b-8ead-eebf98783c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"rb\") as image_file:\n",
    "    b64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "b64_image[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a56bc",
   "metadata": {},
   "source": [
    "Sending the image to the conversation is a *little* different than it was last time, but not too crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dda82d-7e9b-4fca-8741-37020dc2211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What color is the sky in this image?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a2d2a-f732-4c2b-b6df-68fafe3007af",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "The reason why it's good to use the OpenAI-compatible endpoints for all of the models is that it makes it very very very easy to compare their outputs. When you're asking it to write a story you might not care too much, but once you move into data and image analysis you very quickly learn that some models are better than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf5aa3-ba65-4caa-8aa6-f8eca82f0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'city.png'\n",
    "\n",
    "with open(filename, \"rb\") as image_file:\n",
    "    b64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b79f1",
   "metadata": {},
   "source": [
    "Let's analyze this image using OpenAI's GPT-4.1-nano. It's a nice cheap image-capable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e803aec-455c-42ae-aacd-1581a6723336",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"List the stores in this photo\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75def6",
   "metadata": {},
   "source": [
    "How does Google Gemini compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url='https://generativelanguage.googleapis.com/v1beta/openai/', api_key=GEMINI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"List the stores in this photo\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42a7e3",
   "metadata": {},
   "source": [
    "Above we're using `gemini-2.0-flash-lie`, which is a lightweight Gemini model. Let's try again with a more powerful one, `gemini-2.5-flash-preview-05-20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fec6d-40ad-48be-bd1f-8ad32a683355",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url='https://generativelanguage.googleapis.com/v1beta/openai/', api_key=GEMINI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"List the stores in this photo\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6d9a5-b706-4298-8fb0-af7697e67d14",
   "metadata": {},
   "source": [
    "## Structured output\n",
    "\n",
    "You might ask an LLM politely to return data in a certain format, but it's always free to ignore you! [Pydantic](https://docs.pydantic.dev/latest/) allows you to demand more structure. You create a model of what the output should look like and the LLM follows it!\n",
    "\n",
    "> I've been writing Pydantic for a while now, but I usually use an LLM to write the model for me, especially for more complicate ones. It's a lot of boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511aed6f-0a30-4b2c-a22b-601fe74cf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Just ask \"write me a Pydantic model for XXXX\"\n",
    "class ImageDescription(BaseModel):\n",
    "    city_guess: str = Field(\"Best guess of the location in the photograph\")\n",
    "    cars_visible: int = Field(\"Number of visible cars\") \n",
    "    season: Literal['spring', 'summer', 'fall', 'winter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c704b20",
   "metadata": {},
   "source": [
    "Three things change when using structured outputs:\n",
    "\n",
    "1. You add `response_format=` along with your requested structure\n",
    "2. You use `client.beta.chat.completions.parse` to make the request\n",
    "3. The result comes from `completion.choices[0].message.parsed`\n",
    "\n",
    "It's easy to accidentally cut and paste the normal LLM code, add **response_format=**, and end up with an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217b6d0-0a49-4ec4-bed6-bf441c0bfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe this cityscape\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    response_format=ImageDescription,\n",
    ")\n",
    "\n",
    "result = completion.choices[0].message.parsed\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6365a89",
   "metadata": {},
   "source": [
    "Now let's try with **Google Gemini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff88f78-fd39-40d1-8329-d5b2e56ba51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key='AIzaSyCAhb7WnDOfboZN2Bz2TOFpb_VOCtLX5xA'\n",
    ")\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    # gemini-2.5-flash-preview-05-20\n",
    "    # gemini-2.0-flash-lite\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe this cityscape\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    response_format=ImageDescription,\n",
    ")\n",
    "\n",
    "result = completion.choices[0].message.parsed\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f27125",
   "metadata": {},
   "source": [
    "## Analyzing many images\n",
    "\n",
    "While analyzing *one* image is fine and all, usually you want to analyze a whole lot of them! Looking at one image isn't hard, but looking at one thousand makes you an *investigative journalist*.\n",
    "\n",
    "To do this we'll use `glob`, my favorite-named Python tool, to find all of the jpg images in the \"cars\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filenames = glob.glob(\"cars/*.jpg\")\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed88db8",
   "metadata": {},
   "source": [
    "Now let's see what predictions the LLM makes for the country of origin (and some other things) for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Just ask \"write me a Pydantic model for XXXX\"\n",
    "class ImageDescription(BaseModel):\n",
    "    country_guess: str = Field(\"Best guess of the country in the photograph\")\n",
    "    car_make: str\n",
    "    car_model: str\n",
    "    license_plate_number: str\n",
    "    vehicle_category: Literal['car', 'truck', 'suv', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key='AIzaSyCAhb7WnDOfboZN2Bz2TOFpb_VOCtLX5xA'\n",
    ")\n",
    "\n",
    "def ask_llm(filename):\n",
    "    with open(filename, \"rb\") as image_file:\n",
    "        b64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gemini-2.5-flash-preview-05-20\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this cityscape\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format=ImageDescription,\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.parsed\n",
    "    return result\n",
    "\n",
    "results = []\n",
    "for filename in tqdm(filenames):\n",
    "    result = ask_llm(filename)\n",
    "    print(f\"{filename} is {result}\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874898a1",
   "metadata": {},
   "source": [
    "I love to have all of my data live in pandas dataframes, but Pydantic models won't go in nicely! You need to \"dump\" the model to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build into dataframe\n",
    "data = [result.model_dump() for result in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add new columns\n",
    "df['filename'] = filenames\n",
    "df['preview'] = df['filename'].apply(lambda filename: f'<img src=\"{filename}\" width=\"100\"/>')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ba194",
   "metadata": {},
   "source": [
    "My favorite trick is that `preview` column. If you turn on HTML rendering you can use it to look at a little version of your images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(df.to_html(escape=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3710f31-d315-48c0-8e50-6642b0338646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade transformers timm \"scenedetect[opencv]\"==0.6.4\n",
    "%pip install --quiet --upgrade openai \"google-genai\" pydantic pandas \n",
    "%pip install --quiet --upgrade ffmpeg-python pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa173100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEYS GO HERE, ASK SOMA!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee88b6e",
   "metadata": {},
   "source": [
    "# Analyzing videos with AI\n",
    "\n",
    "Now that we've done a little image analysis, now we can look at videos. Remember that **videos are just images and audio and a little bit of time**, and transitioning between the formats is often a convenient way to analyze content effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0634b",
   "metadata": {},
   "source": [
    "## Downloading videos\n",
    "\n",
    "To download the videos we'll use [yt-dlp](https://github.com/yt-dlp/yt-dlp), the most fantastic piece of software ever written. It can download content from *anywhere*. Give it a URL to YouTube, TikTok, Instagram, old weird websites with videos: a-n-y-w-h-e-r-e!\n",
    "\n",
    "It works by doing a little magic scraping, though, so you'll often need to `--upgrade` it with the line below. Even running a version a couple months old is likely to end up with unsuccessful downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb1feb",
   "metadata": {},
   "source": [
    "yt-dlp looks the best on the command line, buuuut I'll keep things cleaner and give you the Python code. I always always always ask a chatbot to write the `ydl_opts` for me, I absolutely cannot remember any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=rDXubdQdJYs\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestvideo[height<=720][vcodec!^=av01]+bestaudio/best[height<=720]',\n",
    "    'outtmpl': '%(id)s.%(ext)s',\n",
    "    'merge_output_format': 'mp4',\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ce141",
   "metadata": {},
   "source": [
    "And now we have our video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076104fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"rDXubdQdJYs.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db624f",
   "metadata": {},
   "source": [
    "## Sending videos to an LLM\n",
    "\n",
    "Google Gemini is the only LLM that can understand videos right now - see\n",
    "https://ai.google.dev/gemini-api/docs/video-understanding for many, many, MANY examples of how to use it.\n",
    "\n",
    "For these features we also need to use Google's `google-genai` package instead of the OpenAI one. Sorry!!! Also note that the package is *not* `google-generativeai`, which is Google's previous AI library. I do not name things around here.\n",
    "\n",
    "We also can't send the video through base64, it would be giant and blow up the planet. Instead we need to upload the video to Google before we analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def upload_video(video_file_name):\n",
    "  video_file = client.files.upload(file=video_file_name)\n",
    "\n",
    "  while video_file.state == \"PROCESSING\":\n",
    "      print('Waiting for video to be processed.')\n",
    "      time.sleep(10)\n",
    "      video_file = client.files.get(name=video_file.name)\n",
    "\n",
    "  if video_file.state == \"FAILED\":\n",
    "    raise ValueError(video_file.state)\n",
    "  print(f'Video processing complete: ' + video_file.uri)\n",
    "\n",
    "  return video_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd56c2",
   "metadata": {},
   "source": [
    "That was all setup: now we upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = upload_video('rDXubdQdJYs.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e080a",
   "metadata": {},
   "source": [
    "After the video is uploaded, we can finally make a request. You can put **anything** in the prompt – again, you should definitely check out the [video understanding](https://ai.google.dev/gemini-api/docs/video-understanding) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "Describe the video.\n",
    "\"\"\"\n",
    "\n",
    "video = video\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    contents=[\n",
    "        video,\n",
    "        prompt,\n",
    "    ],\n",
    "    config=GenerateContentConfig(temperature=0)\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4e126",
   "metadata": {},
   "source": [
    "We're trying to see whether Biden or Trump got more screen time (bias!!! bias!!!) so we'll go the easiest route first: **we can just ask Gemini.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15056d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "Describe the video. Count the number of seconds Trump is alone on the screen and\n",
    "the number of seconds Biden is alone on the screen\n",
    "\"\"\"\n",
    "\n",
    "video = video\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    contents=[\n",
    "        video,\n",
    "        prompt,\n",
    "    ],\n",
    "    config=GenerateContentConfig(temperature=0)\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393be4a9",
   "metadata": {},
   "source": [
    "Great! Wonderful! A response! **But is it correct?**\n",
    "\n",
    "We'll find out later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af5ff0",
   "metadata": {},
   "source": [
    "## Analyzing videos *directly from YouTube*\n",
    "\n",
    "A fun trick when working with YouTube videos is that you can feed them directly to Gemini. You can even analyze like a *two hour long video* this way, it's wild. Only available with Gemini, obviously.\n",
    "\n",
    "> Sometimes you get [weird errors](https://github.com/googleapis/python-genai/issues/378), but the code below seems to work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46001a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Part, HttpOptions\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    http_options=HttpOptions(api_version=\"v1alpha\")\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Transcribe the audio from this video, giving timestamps for salient events in the video. \n",
    "Also provide visual descriptions.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='models/gemini-2.0-flash',\n",
    "    contents= types.Content(\n",
    "        parts=[\n",
    "            types.Part(text=prompt),\n",
    "            types.Part(\n",
    "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=rDXubdQdJYs')\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76484d1c",
   "metadata": {},
   "source": [
    "So now we can do the same \"count the screen time\" thing we did before, but this time instead of uploading the video we can just pass it the YouTube URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Part, HttpOptions\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    http_options=HttpOptions(api_version=\"v1alpha\")\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Describe the video. Count the number of seconds Trump is alone on the screen and\n",
    "the number of seconds Biden is alone on the screen\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='models/gemini-2.0-flash',\n",
    "    contents= types.Content(\n",
    "        parts=[\n",
    "            types.Part(text=prompt),\n",
    "            types.Part(\n",
    "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=rDXubdQdJYs')\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790f7c9",
   "metadata": {},
   "source": [
    "Did the two results match? **How can we verify them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4ce08",
   "metadata": {},
   "source": [
    "## Manually splitting videos into frames\n",
    "\n",
    "Instead of analyzing the video as a holistic object, we can also just break them down into frames. The code below uses ffmpeg – the craziest, most capable video library ever – to pull out a frame of the video every 2 seconds.\n",
    "\n",
    "We can then see who is on screen when and add it up for a rough count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331aa5c-690b-46f7-8237-0b1934159066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "from pathlib import Path\n",
    "\n",
    "interval_seconds = 2\n",
    "output_dir = Path(\"debate\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_pattern = str(output_dir / \"frame-%03d.jpg\")\n",
    "\n",
    "(\n",
    "    ffmpeg\n",
    "    .input(\"rDXubdQdJYs.mp4\")\n",
    "    .output(output_pattern, vf=f'fps=1/{interval_seconds}')\n",
    "    .run(capture_stdout=True, capture_stderr=True)\n",
    ")\n",
    "\n",
    "print(f\"Frames saved to '{output_dir}' (1 every {interval_seconds} seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11735969",
   "metadata": {},
   "source": [
    "Let's look at a single image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9eb0b-d4d8-47ec-9f58-72f57ad4816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"debate/frame-004.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59e3a4",
   "metadata": {},
   "source": [
    "*We* know who that is, but we're lazy! Let's leverage our skills from before and use Pydantic and an LLM to classify the image as being either:\n",
    "\n",
    "- Joe Biden\n",
    "- Donald Trump\n",
    "- Both/None/Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f21dda-61aa-4ee0-8a95-9e35f37b79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "import base64\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    person: Literal['Joe Biden', 'Donald Trump', 'Both/None/Other'] = Field(\"What politician is in this image?\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "def ask_llm(filename):\n",
    "    with open(filename, \"rb\") as image_file:\n",
    "        b64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gemini-2.5-flash-preview-05-20\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{b64_image}\" } }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format=ImageDescription\n",
    "    )\n",
    "    \n",
    "    result = completion.choices[0].message.parsed\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c4137",
   "metadata": {},
   "source": [
    "That code allows us to use `ask_llm` with a filename to get a response. How well does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llm('debate/frame-004.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a143e-70e0-403d-837e-bc557d0a2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('debate/frame-007.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc2cf-70a5-4e1a-821f-bbe5816541bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llm('debate/frame-007.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd61f49-fe43-44cc-b13b-0bae9b065e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('debate/frame-001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e72f3-b3a9-4bf1-a99c-d3b81fd23ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llm('debate/frame-001.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88d1b9",
   "metadata": {},
   "source": [
    "Looking pretty good to me! Now let's **process all the frames**, just like we did in the last notebook when working with individual, unrelated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec3d93-5df0-4e66-b8be-0c549c2e41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filenames = glob.glob(\"debate/*.jpg\")\n",
    "filenames = sorted(filenames)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc6244-0f9e-4b5b-8a47-aeee23595078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for filename in tqdm(filenames):\n",
    "    result = ask_llm(filename)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0defd-e468-4c15-b620-508e620dfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build into dataframe\n",
    "data = [result.model_dump() for result in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add new columns\n",
    "df['filename'] = filenames\n",
    "df['preview'] = df['filename'].apply(lambda filename: f'<img src=\"{filename}\" width=\"100\"/>')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411b0e0",
   "metadata": {},
   "source": [
    "Let's count them up and see how it looks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbb5af-1431-4cd6-b294-47c5a6536399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['person'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775149d",
   "metadata": {},
   "source": [
    "Does that match what Gemini said? And if not, how might we still be wrong?\n",
    "\n",
    "...and while we're at it: **let's verify the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12225e4d-f88f-4621-96a2-bcd190144dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(df.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96840536",
   "metadata": {},
   "source": [
    "## Splitting on scenes\n",
    "\n",
    "If you do this kind of analysis on anything that isn't a quick-cuts YouTube short, you might end up spending a *lot* of time and money on the process. Imagine if Biden was on the screen for 6 minutes – that's 360 images you're processing for *no good reason*.\n",
    "\n",
    "So let's try another approach: [pySceneDetect](https://github.com/Breakthrough/PySceneDetect) is an incredible library that uses basis algorithms to split videos into scenes. It can save CSV files describing the splits, save an array of images from each scene, make a nice little HTML guide, cut out image clips, all sorts of things. You should read the documentation!\n",
    "\n",
    "Below we're going to detect a change in scenes using the `ContentDetector` and save some details about each scene (images, a csv, and an HTML guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16411653-5399-41da-a2fa-2e18986f06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenedetect import detect, ContentDetector, open_video\n",
    "from scenedetect.scene_manager import save_images, write_scene_list_html, write_scene_list\n",
    "import os\n",
    "\n",
    "video_path = \"rDXubdQdJYs.mp4\"\n",
    "output_dir = \"debate-scenes\"\n",
    "image_width = 300\n",
    "num_images = 5\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "scene_list = detect(video_path, ContentDetector())\n",
    "\n",
    "video_stream = open_video(video_path)\n",
    "\n",
    "save_images(\n",
    "    scene_list,\n",
    "    video_stream,\n",
    "    output_dir=output_dir,\n",
    "    num_images=num_images,\n",
    "    width=image_width,\n",
    ")\n",
    "\n",
    "write_scene_list_html(\n",
    "    scene_list=scene_list,\n",
    "    output_html_filename=os.path.join(output_dir, \"scenes.html\"),\n",
    "    image_width=image_width,\n",
    ")\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"scenes.csv\")\n",
    "with open(csv_path, \"w\", encoding=\"utf-8\") as csvfile:\n",
    "    write_scene_list(\n",
    "        output_csv_file=csvfile,\n",
    "        scene_list=scene_list,\n",
    "        include_cut_list=False,\n",
    "    )\n",
    "\n",
    "# Print start/end timecodes for each scene\n",
    "for idx, (start, end) in enumerate(scene_list, 1):\n",
    "    print(f\"Scene {idx}: {start.get_timecode()} – {end.get_timecode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe348d",
   "metadata": {},
   "source": [
    "If you're running this on your own computer, you can open up the HTML file yourself. If not..................... it's fine, just watch me.\n",
    "\n",
    "We can also get details on every scene – including length – by looking at the CSV file it built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f0175-1a6f-4252-8f24-41cd93acd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"debate-scenes/scenes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a66e2",
   "metadata": {},
   "source": [
    "In order to see who is talking in each scene, I think taking the middle image of the scene should be a reasonable representation. If we have 5 images from each scene, the 3rd of each should be what we're looking for.\n",
    "\n",
    "We're using glob again below, but adjusting the pattern to only take the `-03.jpg` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c82127-81f5-4e89-828c-2ca997da2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get the third image of each scene\n",
    "filenames = glob.glob(\"debate-scenes/*-03.jpg\")\n",
    "filenames = sorted(filenames)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598faef",
   "metadata": {},
   "source": [
    "Now we can ask the LLM again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c3b1e-2d0b-4373-83f0-61dc49940205",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for filename in filenames:\n",
    "    answer = ask_llm(filename)\n",
    "    answers.append(answer)\n",
    "    print(f\"{filename} received response {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654325b",
   "metadata": {},
   "source": [
    "It was less time, even though it might have been more work. How did it turn out? Let's *not really pay attention yet*, we have more information to combine with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558fa4b-294a-416b-b768-81566434ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [obj.model_dump() for obj in answers]\n",
    "\n",
    "results = pd.DataFrame(dicts)\n",
    "results['filename'] = filenames\n",
    "results['preview'] = results.filename.apply(lambda filename: f'<img src=\"{filename}\" width=\"100\"/>')\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459def4b",
   "metadata": {},
   "source": [
    "### Combining results with scene data\n",
    "\n",
    "By adding in the scene data we can get a *lot* of additional information beyond just \"there was a scene with this image.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432fb8e-a9ca-4786-abfb-b5a70965ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.join(results)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f437a-66a2-4089-a4ab-cf4daae64424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "# Grab five of them\n",
    "display.HTML(merged.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995b94f",
   "metadata": {},
   "source": [
    "And now, most importantly: we can count up the seconds, grouped by each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada94dd-be64-400f-9246-83ce55b912f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby('person')['Length (seconds)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47197f87-c9cb-47a7-84b6-d94dd1470f10",
   "metadata": {},
   "source": [
    "## Local models: faster, more private, more information\n",
    "\n",
    "Let's say you're doing something like this, but *it's a bunch of secret stuff*. You don't want to share it with OpenAI or Google or anyone like that. You're also tired of giving money to Big Tech, and every request takes way too long!\n",
    "\n",
    "The answer is **local models!** As long as your question isn't *too complicated*, code that runs on your own computer can probably handle it. You'll want to investigate [zero-shot classification models](https://huggingface.co/tasks/zero-shot-image-classification), which means \"this model can put things into categories without you teaching it anything.\"\n",
    "\n",
    "Vision-capable LLMs are huge, giant beasts that you could never hope to run. But once you start to move beyond an all-powerful single tool, a [million tiny tools](https://huggingface.co/models) appear that [can do much of what you're looking for](https://huggingface.co/tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86684498-9164-4059-aa92-31c41635a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "filename = 'debate-scenes/rDXubdQdJYs-Scene-002-03.jpg'\n",
    "image = Image.open(filename)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed104c",
   "metadata": {},
   "source": [
    "[OpenAI's clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) isn't *new*, but it will work fine for our cases (and it's plenty popular). Let's see how it does on one of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff441e-9e4a-4066-aff3-7820a28fdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a classifier\n",
    "detector = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-large-patch14\") \n",
    "\n",
    "# Use the classifier\n",
    "results = detector(image, candidate_labels=[\"donald trump\", \"joe biden\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794840e",
   "metadata": {},
   "source": [
    "Amazing???? And we get a confidence score????? And it was so fast???????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c7a7c",
   "metadata": {},
   "source": [
    "### Run the classifier on ALL the images\n",
    "\n",
    "Well, not *all* the images, just the middle image. But close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30815864-a500-4ea5-808f-084bbac1352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get the third image of each scene\n",
    "filenames = glob.glob(\"debate-scenes/*-03.jpg\")\n",
    "filenames = sorted(filenames)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb2687",
   "metadata": {},
   "source": [
    "Take a close look at the code below: if the confidence score is below 98% we mark it as \"unknown.\" This is the kind of flexibility you get when you move away from LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dda4d9-e7bc-49a4-87ae-794cac95c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Build the detector\n",
    "detector = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-large-patch14\") \n",
    "\n",
    "answers = []\n",
    "for filename in filenames:\n",
    "    # Open the image\n",
    "    image = Image.open(filename)\n",
    "\n",
    "    # See the results\n",
    "    results = detector(image, candidate_labels=[\"donald trump\", \"joe biden\"])\n",
    "    top_result = results[0]\n",
    "\n",
    "    top_score = top_result['score']\n",
    "    # Be 98% sure\n",
    "    if top_result['score'] > 0.98:\n",
    "        label = top_result['label']\n",
    "    else:\n",
    "        label = 'unknown'\n",
    "\n",
    "    answers.append({\n",
    "        'filename': filename,\n",
    "        'label': label,\n",
    "        'score': top_score,\n",
    "        'preview': f'<img src=\"{filename}\" width=\"100\"/>'\n",
    "    })\n",
    "\n",
    "    print(filename, label, top_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64552f56",
   "metadata": {},
   "source": [
    "Now let's do the typical combining..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593a503-961e-475d-8418-26567cb87f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(answers)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386b7a5-168b-476c-9ed0-90619aff44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"debate-scenes/scenes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6dfdf2",
   "metadata": {},
   "source": [
    "...and how's it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ef271-cd52-458f-8eeb-7771db8805d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "merged = results_df.join(df)\n",
    "\n",
    "HTML(merged.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3b510-fd28-423f-9c88-9bb54d98ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(\"merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73367a",
   "metadata": {},
   "source": [
    "And now, the **final calculation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b9cd3-f718-46d4-8581-d3ca758b1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby('label')['Length (seconds)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4e9c5",
   "metadata": {},
   "source": [
    "We can also nudge up the required confidence score if we really want to be *certain* certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a93236-6f50-489e-9fe9-670d9ab5d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.query('score > 0.99').groupby('label')['Length (seconds)'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
